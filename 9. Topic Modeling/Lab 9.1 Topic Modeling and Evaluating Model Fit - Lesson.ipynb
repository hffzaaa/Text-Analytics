{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acf6afed",
   "metadata": {},
   "source": [
    "# Example 1\n",
    "\n",
    "documents = [\n",
    " \"Rafael Nadal Joins Roger Federer in Missing U.S. Open\",\n",
    " \"Rafael Nadal Is Out of the Australian Open\",\n",
    " \"Biden Announces Virus Measures\",\n",
    " \"Biden's Virus Plans Meet Reality\",\n",
    " \"Where Biden's Virus Plan Stands\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdfd273",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba71ef1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Hafizatul\n",
      "[nltk_data]     A'fifah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Hafizatul\n",
      "[nltk_data]     A'fifah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Hafizatul\n",
      "[nltk_data]     A'fifah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text preprocessing\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Topic modeling\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1cde16",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e76f8b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Rafael Nadal Joins Roger Federe in Missing U.S. Open\",\n",
    "    \"Rafael Nadal Is Out of the Australian Open\",\n",
    "    \"Biden Announces Virus Measures\",\n",
    "    \"Biden's Virus Plans Meet Reality\",\n",
    "    \"Where Biden's Virus Plan Stands\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414956ca",
   "metadata": {},
   "source": [
    "#### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "286dcebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['rafael', 'nadal', 'join', 'roger', 'federe', 'missing', 'open'],\n",
       " ['rafael', 'nadal', 'australian', 'open'],\n",
       " ['biden', 'announces', 'virus', 'measure'],\n",
       " ['biden', 'virus', 'plan', 'meet', 'reality'],\n",
       " ['biden', 'virus', 'plan', 'stand']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english')) # Create a set of English stopwords\n",
    "lemmatizer = WordNetLemmatizer() # Initialize a WordNet lemmatizer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower()) # Tokenize the text into words and convert to lowercase\n",
    "    tokens = [token for token in tokens if token.isalnum()] # Filter out non-alphanumeric tokens\n",
    "    tokens = [token for token in tokens if token not in stop_words]  # Remove stopwords from the tokens\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens] # Lemmatize each token\n",
    "    return tokens # Return the preprocessed tokens\n",
    "\n",
    "\n",
    "preprocessed_documents = [preprocess_text(doc) for doc in documents] # Preprocess each document in the list\n",
    "preprocessed_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a2cd7e",
   "metadata": {},
   "source": [
    "#### Create Document-Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3399d63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(preprocessed_documents) # Create a Gensim Dictionary object from the preprocessed documents\n",
    "corpus = [dictionary.doc2bow(doc) for doc in preprocessed_documents] # Convert each preprocessed document into a bag-of-words representation using the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051318f5",
   "metadata": {},
   "source": [
    "#### Run LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb91a8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LdaModel(corpus, num_topics=2, id2word=dictionary, passes=15) # Train an LDA modelon the corpus with 2 topics using Gensim's LdaModel class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841b7026",
   "metadata": {},
   "source": [
    "#### Interpret Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3395b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty list to store dominant topic labels for each document\n",
    "article_labels = []\n",
    "\n",
    "# iterate over each processed document\n",
    "for i, doc in enumerate(preprocessed_documents):\n",
    "    # for each document, convert to bag-of-words representation\n",
    "    bow = dictionary.doc2bow(doc)\n",
    "    # get list of topic probabilities\n",
    "    topics = lda_model.get_document_topics(bow)\n",
    "    # determine topic with highest probability\n",
    "    dominant_topic = max(topics, key=lambda x: x[1])[0]\n",
    "    # append to the list\n",
    "    article_labels.append(dominant_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9406499f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table with Articles and Topic:\n",
      "                                             Article  Topic\n",
      "0  Rafael Nadal Joins Roger Federe in Missing U.S...      0\n",
      "1         Rafael Nadal Is Out of the Australian Open      0\n",
      "2                     Biden Announces Virus Measures      1\n",
      "3                   Biden's Virus Plans Meet Reality      1\n",
      "4                    Where Biden's Virus Plan Stands      1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\"Article\": documents, \"Topic\": article_labels})\n",
    "\n",
    "print(\"Table with Articles and Topic:\")\n",
    "print(df)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "520a2097",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Terms for Each Topic:\n",
      "Topic 0:\n",
      "- \"open\" (weight: 0.131)\n",
      "- \"nadal\" (weight: 0.131)\n",
      "- \"rafael\" (weight: 0.131)\n",
      "- \"missing\" (weight: 0.079)\n",
      "- \"federe\" (weight: 0.079)\n",
      "- \"roger\" (weight: 0.079)\n",
      "- \"join\" (weight: 0.079)\n",
      "- \"australian\" (weight: 0.079)\n",
      "- \"biden\" (weight: 0.027)\n",
      "- \"virus\" (weight: 0.027)\n",
      "\n",
      "Topic 1:\n",
      "- \"virus\" (weight: 0.166)\n",
      "- \"biden\" (weight: 0.166)\n",
      "- \"plan\" (weight: 0.119)\n",
      "- \"meet\" (weight: 0.071)\n",
      "- \"reality\" (weight: 0.071)\n",
      "- \"stand\" (weight: 0.071)\n",
      "- \"measure\" (weight: 0.071)\n",
      "- \"announces\" (weight: 0.071)\n",
      "- \"rafael\" (weight: 0.024)\n",
      "- \"australian\" (weight: 0.024)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Top Terms for Each Topic:\")\n",
    "for idx, topic in lda_model.print_topics():\n",
    "    print(f\"Topic {idx}:\")\n",
    "    terms = [term.strip() for term in topic.split(\"+\")]\n",
    "    for term in terms:\n",
    "        weight, word = term.split(\"*\")\n",
    "        print(f\"- {word.strip()} (weight: {weight.strip()})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cbe414",
   "metadata": {},
   "source": [
    "Topic 0 seems to be related around politics and virus, where the weight of terms like \"biden\" and \"virus\" are particularly high, indicating their significance in this topic.\n",
    "\n",
    "Topic 1 seems to be related to tennis, where the weight of terms like \"nadal\" and \"rafael\" are relatively high, suggesting a strong association with this topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f730566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Coherence Score (C_V): 0.3801\n"
     ]
    }
   ],
   "source": [
    "# Calculate the coherence score for the LDA model\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=preprocessed_documents, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "# Display the score\n",
    "print(f'Topic Coherence Score (C_V): {coherence_lda:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
